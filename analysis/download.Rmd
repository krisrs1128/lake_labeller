---
title: "Download Sentinel 2 Imagery"
output: html_notebook
params:
  download_set: 1
---

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
library(glue)
library(rstac)
library(lakes)
library(lubridate)
library(reticulate)
library(sf)
library(tidyverse)
library(terra)
use_condaenv("lakes_labeller")
data(glaciers)
```

```{r}
subsets <- expand.grid(
  year = 2021,
  month = seq(1, 12, by = 4),
  n_months = 4,
  GLIMS_ID = as.character(glaciers$GLIMS_ID)
) %>%
  arrange(year, month) %>%
  mutate(download_set = rep(row_number(), each = 20, length.out = n())) %>%
  filter(download_set == params$download_set)
```

```{r}
links <- list()
for (i in seq_len(nrow(subsets))) {
  # extract query parameters for current row of subsets
  gl_id <- subsets$GLIMS_ID[i]
  start_date <- ymd(str_c(subsets$year[i], "-", subsets$month[i], "-01"))
  end_date <- start_date %m+% months(subsets$n_months[i])
  date_range <- paste(c(start_date, end_date), collapse = "/")
  
  # get metadata for the current glacier (if there are any items returned)
  cur_glacier <- filter(glaciers, GLIMS_ID == gl_id)
  candidates <- scenes_metadata(date_range, st_bbox(cur_glacier), max_cloud = 20)
  if (length(candidates) == 0) next
  links[[gl_id]] <- candidates %>%
    mutate(date = date(as_datetime(p.datetime)))
  
  # download all timepoints for this glacier
  timepoints <- unique(links[[gl_id]]$id)
  for (j in seq_along(timepoints)) {
    out_path <- glue("{gl_id}_{links[[gl_id]]$date[1]}.tif")
    cur_links <- filter(links[[gl_id]], id == timepoints[j])
    bbox <- st_bbox(st_buffer(cur_glacier, 0.5))
    result <- read_windows(cur_links$link, bbox)
    
    if (length(result) > 0) {
      extent <- ext(bbox[1], bbox[3], bbox[2], bbox[4])
      writeRaster(result, out_path, overwrite = TRUE)
    }
  }
  Sys.sleep(5)
}
```

```{r}
bind_rows(links, .id = "GL_ID") %>%
  write_csv(glue("metadata_{params$download_set}.csv"))
```

   #   rast(result, crs="EPSG:4326", extent=extent) %>%